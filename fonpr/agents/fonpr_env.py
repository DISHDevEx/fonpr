'''
Class defining the Gymnasium type environment for FONPR agents.
'''

import gymnasium as gym
import pandas as pd
import numpy as np
from gymnasium import spaces
from time import sleep

from advisors import PromClient
from utilities import prom_query_rl_upf_experiment1, ec2_cost_calculator
from action_handler import ActionHandler, get_token

class FONPR_Env(gym.Env):
    metadata = {"render_modes": []}

    def __init__(self, render_mode=None, window=15, sample_rate=4, obs_period=5):
        # An observation in this case is generated by Prometheus logs covering the past 15 minutes
        self.window = window # How far back in time does the observation look, in minutes
        self.sample_rate = sample_rate # How many observation samples are collected per minute
        self.samples = window * sample_rate
        self.obs_period = obs_period # How frequently does a new observation occur
        
        # States we are observing consist of "Large instance On", "Small instance On", "Throughput"
        self.obs_space = spaces.Box(
            low=np.tile(np.array([0, 0, 0]).astype("float32"), (self.samples,1)), 
            high=np.tile(np.array([1, 1, np.finfo(np.float32).max - 1]).astype("float32"), (self.samples,1)), 
            shape=(self.samples, 3)
            )

        # We have 3 actions, corresponding to "Transition to Large", "Transition to Small", "Do Nothing"
        self.action_space = spaces.Discrete(3)

        assert render_mode is None or render_mode in self.metadata["render_modes"]
        self.render_mode = render_mode

    def _get_obs(self):
        # Request query from Prometheus
        prom_client_advisor = PromClient("http://10.0.104.52:9090")
        prom_client_advisor.set_queries_by_function(prom_query_rl_upf_experiment1())
        prom_response = prom_client_advisor.run_queries()
        
        return prom_response

    def _get_info(self):
        # Provide information on state, action, and reward?
        return None
        
    def reset(self, seed=None, options=None):
        # We need the following line to seed self.np_random
        super().reset(seed=seed)

        observation = self._get_obs()
        info = self._get_info()

        return observation, info

    def step(self, action):
        gh_url="https://github.com/DISHDevEx/napp/blob/aakash/hpa-nodegroups/napp/open5gs_values/5gSA_no_ues_values_with_nodegroups.yaml",
        dir_name="napp"
        
        if action == 0: # Transition to Large instance
            hndl = ActionHandler(get_token(), gh_url, dir_name, {"target_pod": "upf", "values": "Large"})
            hndl.fetch_update_push()
        elif action == 1: # Transition to Small instance
            hndl = ActionHandler(get_token(), gh_url, dir_name, {"target_pod": "upf", "values": "Small"})
            hndl.fetch_update_push()
        elif action == 2: # Do Nothing
            pass
            
        sleep(self.obs_period * 60) # Sleep for observation period before retrieving next observation
        
        rxtx_value = 3.33e-9 # Rough estimate of dollars per bit over the network
        li_cost = ec2_cost_calculator("m4.large") # Cost of large instance in dollars per hour
        si_cost = ec2_cost_calculator("t3.medium") # Cost of small instance in dollars per hour
        
        observation = self._get_obs()
        # reward over window: revenue generated by rx/tx on network ($) - cost of running large instance ($) - cost of running small instance ($)
        reward = rxtx_value * 8 * np.sum(self.obs_space[2]) \
            -(li_cost / 60 * np.sum(self.obs_space[0]) / len(self.obs_space) * self.window) \
            -(si_cost / 60 * np.sum(self.obs_space[1]) / len(self.obs_space) * self.window)
        info = self._get_info()

        # return observation, reward, terminated, truncated, info
        return observation, reward, 0, 0, info

    def render(self):
        ...

    def close(self):
        ...