'''
Class defining the Gymnasium type environment for FONPR agents.
'''

import gymnasium as gym
from gymnasium import spaces
import numpy as np
from time import sleep

class FONPR_Env(gym.Env):
    metadata = {"render_modes": []}

    def __init__(self, render_mode=None, window=15, sample_rate=4, obs_period=5):
        # An observation in this case is generated by Prometheus logs covering the past 15 minutes
        self.window = window # How far back in time does the observation look, in minutes
        self.sample_rate = sample_rate # How many observation samples are collected per minute
        self.obs_period = obs_period # How frequently does a new observation occur
        
        # States we are observing consist of "Large instance On", "Small instance On", "Throughput"
        samples = window * sample_rate
        self.obs_space = spaces.Box(
            low=np.tile(np.array([0, 0, 0]), (samples,1)).transpose(), 
            high=np.tile(np.array([1, 1, np.finfo(np.float32).max - 1]), (samples,1)).transpose(), 
            shape=(3, samples)
            )

        # We have 3 actions, corresponding to "Transition to Large", "Transition to Small", "Do Nothing"
        self.action_space = spaces.Discrete(3)

        assert render_mode is None or render_mode in self.metadata["render_modes"]
        self.render_mode = render_mode

    def _get_obs(self):
        # Request query from Prometheus
        pass

    def _get_info(self):
        # Provide information on state, action, and reward?
        pass

    def reset(self, seed=None, options=None):
        # We need the following line to seed self.np_random
        super().reset(seed=seed)

        # observation = self._get_obs()
        # info = self._get_info()

        # return observation, info

    def step(self, action):
        if action == 0: # Transition to Large instance
            pass
        elif action == 1: # Transition to Small instance
            pass
        elif action == 2: # Do Nothing
            pass
            
        sleep(self.obs_period * 60) # Sleep for observation period before checking state again
        
        rxtx_value = .33e-9 # Roughly equivalent to dollars per bit over the network
        li_cost = .17 # Cost of large instance in dollars per hour
        si_cost = .05 # Cost of small instance in dollars per hour
        
        # observation = self._get_obs()
        reward = rxtx_value * np.sum(self.obs_space[2]) * 60
        # info = self._get_info()

        # return observation, reward, terminated, truncated, info

    def render(self):
        ...

    def close(self):
        ...