'''
Class defining the Gymnasium type environment for FONPR agents.
'''

import gymnasium as gym
import pandas as pd
import numpy as np
import logging
from gymnasium import spaces
from time import sleep

from advisors import PromClient
from utilities import prom_query_rl_upf_experiment1, ec2_cost_calculator
from action_handler import ActionHandler, get_token


class FONPR_Env(gym.Env):
    metadata = {"render_modes": []}

    def __init__(self, render_mode=None, window=15, sample_rate=4, obs_period=5):
        # An observation in this case is generated by Prometheus logs covering the past 15 minutes
        self.window = window # How far back in time does the observation look, in minutes
        self.sample_rate = sample_rate # How many observation samples are collected per minute
        self.samples = window * sample_rate
        self.obs_period = obs_period # How frequently does a new observation occur
        
        # Temporary use until instance presence can be tracked; allows approximate reward calculation in the absense of real time data.
        self.instance_size = "Large"
        
        # States we are observing consist of "Large instance On", "Small instance On", "Throughput"
        # TODO: incorporate instance presence tracking for observation and reward function
        self.obs_space = spaces.Box(
            low=np.tile(np.array([0, 0, 0]).astype("float32"), (self.samples,1)), 
            high=np.tile(np.array([1, 1, np.finfo(np.float32).max - 1]).astype("float32"), (self.samples,1)), 
            shape=(self.samples, 3)
            )

        # We have 3 actions, corresponding to "NOOP", Transition to Large", "Transition to Small"
        self.action_space = spaces.Discrete(3)

        assert render_mode is None or render_mode in self.metadata["render_modes"]
        self.render_mode = render_mode

    def _get_obs(self):
        # Request query from Prometheus
        prom_client_advisor = PromClient("http://10.0.104.52:9090")
        prom_client_advisor.set_queries_by_function(prom_query_rl_upf_experiment1())
        prom_response = prom_client_advisor.run_queries()
        
        df = pd.DataFrame(prom_response[0]['values']) # Create dataframe on throughput values
        df = df.set_index(0) # Use timestamps for index
        df.index = pd.to_datetime(df.index, unit='s')
        
        df[1] = df[1].astype(float)
        df[1] = df[1] - df.iloc[0,0] # Normalize all throughput to value at first timestamp
        df = df.interpolate() # Linear interpolation for any missing values
        df = df.resample(f'{int(60/self.sample_rate)}s').mean()
        df = df.interpolate() # Linear interp again for any NaNs created by resample
        
        through_vals = df[1].values # Get numpy array for processed throughput values
        
        # Prepend zeroes if less samples present than required
        if len(through_vals) != self.samples:
            through_vals = np.insert(through_vals, 0, np.zeros(self.samples - len(through_vals)))
        
        # Process pod info
        # TODO: for all pods found over timeframe, isolate host_ip and timestamps
        # TODO: for all pods found over timeframe, map host_ip to instance-type
        # TODO: for each state variable ('Large instance On', 'Small instance On'), use instance-type and timesteps to map boolean
        # TODO: reshape to get vector of obs_space specified length
        # TODO: interpolate in case reshape creates nulls
        
        return prom_response

    def _get_info(self):
        # Provide information on state, action, and reward?
        return "TBD"
        
    def reset(self, seed=None, options=None):
        # We need the following line to seed self.np_random
        super().reset(seed=seed)

        observation = self._get_obs()
        info = self._get_info()

        return observation, info

    def step(self, action):
        gh_url="https://github.com/DISHDevEx/napp/blob/aakash/hpa-nodegroups/napp/open5gs_values/5gSA_no_ues_values_with_nodegroups.yaml",
        dir_name="napp"
        
        if action == 0: # No-Op; do nothing
            logging.info('No action taken for this cycle.')
            pass
        elif action == 1: # Transition to Large instance
            hndl = ActionHandler(get_token(), gh_url, dir_name, {"target_pod": "upf", "values": "Large"})
            hndl.fetch_update_push()
        elif action == 2: # Transition to Small instance
            hndl = ActionHandler(get_token(), gh_url, dir_name, {"target_pod": "upf", "values": "Small"})
            hndl.fetch_update_push()
            
        # sleep(self.obs_period * 60) # Sleep for observation period before retrieving next observation
        sleep(10) # For testing
        
        rxtx_value = 3.33e-9 # Rough estimate of dollars per byte over the network
        li_cost = ec2_cost_calculator("m4.large") # Cost of large instance in dollars per hour
        si_cost = ec2_cost_calculator("t3.medium") # Cost of small instance in dollars per hour
        
        observation = self._get_obs()
        # reward over window: revenue generated by rx/tx on network ($) - cost of running large instance ($) - cost of running small instance ($)
        reward = rxtx_value * np.sum(observation) #\
            # -(li_cost / 60 * np.sum(self.obs_space[1]) / len(self.obs_space) * self.window) \
            # -(si_cost / 60 * np.sum(self.obs_space[2]) / len(self.obs_space) * self.window)
        info = self._get_info()

        # return observation, reward, terminated, truncated, info
        return observation, reward, 0, 0, info

    def render(self):
        ...

    def close(self):
        ...
