{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture stream from fluentbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-a14d8b4137e4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-a14d8b4137e4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sudo apt install  -y default-jre\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sudo apt install  -y default-jre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4d6a2063c636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import json\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a local StreamingContext with two working thread and batch interval of 15 second\n",
    "# sc = SparkContext(\"local[2]\", \"responsStream\")\n",
    "# ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "# ssc.start() \n",
    "# ssc.awaitTermination() \n",
    "# lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "# logging.info(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess a .gz file to isolate one pods worth of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gz2df(path):\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        \"\"\"\n",
    "        Convert .gz log files into dataframe\n",
    "        \"\"\"\n",
    "        content = f.read().decode('utf-8').split('\\n')\n",
    "        list_rows = []\n",
    "\n",
    "        for i in range(len(content)-1):\n",
    "            row = content[i].split(' ')\n",
    "            list_rows.append(row)\n",
    "        f.close()\n",
    "\n",
    "    df = pd.DataFrame(list_rows, columns=['log_timestamp', 'data'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_type(x):\n",
    "    idx1 = x.find('\"Type\":') + 8\n",
    "    idx2 = x.find(',', idx1) - 1\n",
    "    rec_type = x[ idx1:idx2 ]\n",
    "\n",
    "    return rec_type\n",
    "\n",
    "def filter_by_rec_type(df, rec_type): \n",
    "    return df[df.rec_type == rec_type]\n",
    "\n",
    "def explode_df(df):\n",
    "    return pd.concat([df.log_timestamp, pd.json_normalize(df.data)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PodNet\n",
      "NodeNet\n",
      "NodeFS\n",
      "Container\n",
      "Pod\n",
      "NodeDiskIO\n",
      "Node\n"
     ]
    }
   ],
   "source": [
    "streamed_data_file = \"./node1.gz\"\n",
    "\n",
    "df = gz2df(streamed_data_file)\n",
    "df['rec_type'] = df.data.apply(get_type)\n",
    "df['data'] = df.data.apply(json.loads)\n",
    "\n",
    "\n",
    "\n",
    "list_of_df_rectypes = {}  \n",
    "\n",
    "for rec_type in df.rec_type.unique():\n",
    "    print(rec_type)\n",
    "    rec_type_df = filter_by_rec_type(df, rec_type)\n",
    "    df_exploded = explode_df(rec_type_df)\n",
    "    list_of_df_rectypes[rec_type]=df_exploded\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pod = list_of_df_rectypes[\"Pod\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upf data exists\n"
     ]
    }
   ],
   "source": [
    "if(\"open5gs-upf\" in df_pod.PodName.unique()):\n",
    "    print(\"upf data exists\")\n",
    "\n",
    "\n",
    "upf_df = df_pod[df_pod[\"PodName\"]==\"open5gs-upf\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "## cleaning up the nans in the dataframe\n",
    "\n",
    "print(upf_df['pod_cpu_usage_total'].isna().sum())\n",
    "print(len(upf_df))\n",
    "\n",
    "upf_df = upf_df.dropna(subset=['pod_cpu_usage_total', 'pod_memory_max_usage' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting the limits and requests\n",
    "limit_cpu = max( upf_df[[\"pod_cpu_usage_total\"]].values.tolist() )\n",
    "request_cpu = np.mean( upf_df[[\"pod_cpu_usage_total\"]].values.tolist() ) \n",
    "\n",
    "limit_memory =   max( upf_df[[\"pod_memory_max_usage\"]].values.tolist() )\n",
    "request_memory = np.mean( upf_df[[\"pod_memory_max_usage\"]].values.tolist() ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'limit_cpu: {limit_cpu}')\n",
    "logging.info(f'request_cpu: {request_cpu}')\n",
    "logging.info(f'limit_memory: {limit_memory}')\n",
    "logging.info(f'request_memory: {request_memory}')\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
